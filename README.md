# Programming.in.th Grader

## Directories
The general directory hierarchy of the grader is as follows:
- Base directory
    - Task 1 (directory)
        - manifest.json
        - inputs (directory)
        - outputs (directory)
        - solutions (directory)
        - user_bin (directory)
        - checker
    - Task 2 (directory)
        - manifest.json
        ...
    - Task 3 (directory)
        ...

What does each directory/file do?
* manifest.json - contains all the meta data about a task (see Manifest Format)
* inputs - stores input files for each test case
* outputs - stores output generated by the user's program on each teach
* solutions - stores solution files for each test case
* user_bin - temporary storage for each submissions' executables
* checker - an executable checker script (see Checker)

**Remark 1:** outputs and user_bin directories do not need to be manually created since the grader automatically creates these if they don't exist.
**Remark 2:** the checker script and manifest file must be stored in the root of the task's directory and have the exact names "checker" and "manifest.json" (without quotes)

## Manifest Format

The manifest file for each task is stored in the JSON format as "manifest.json", and placed at the root of the task's directory.

All fields are required, which include:
* ID: A string indicating the task ID. Must match the task's directory name.
* TimeLimit: An integer indicating the task's time limit
* MemoryLimit: A floating-point number indicating the task's memory limit
* LangSupport: An array indicating which languages are supported. These must be the standard file extensions (e.g. "cpp", "py", "java", "js", etc.) 
* TestInputs: An array of strings indicating the name of the input file for each test case relative to the "inputs" directory
* TestSolutions: An array of string indicating the name of the solution file for each test case relative to the "solutions" directory
* Groups: An array of objects, each denoting one test group. Each group has the following properties:
    * FullScore: A floating-point number indicating the full score of that test group
    * TestIndices: An object that indicates the continuous range of indices of tests in TestInputs and TestSolutions that belong to the test group
        * Start: An integer denoting the starting index of the test index range (**inclusive**)
        * End: An integer denoting the ending index of the test index range (**exclusive**)
* CompileCommands: An object indicating the shell commands to compile for each language. Each key is a supported language (from the LangSupport array) and the respective value is an array of commands and arguments to be run. For supported interpreted languages, you still need to provide the key and an empty array to indicate that there are no compile commands. To denote the user's source code, simply add "$USER_SRC" as an element in the array. If there are multiple sources, simple add "$USER_SRC" as many times as there are source files (i.e. use "$USER_SRC" whenever a source file needs to be specified), since the grader will automatically load this in from the API. You must also add "$USER_BIN" in the array to denote the argument that indicates the path to the output executable.

Here is a sample manifest.json file:
```json
{
	"ID": "rectsum",
	"TimeLimit": 10,
	"MemoryLimit": 262144,
	"LangSupport": ["cpp", "python"],
	"TestInputs":["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20"],
	"TestSolutions":["01.a", "02.a", "03.a", "04.a", "05.a", "06.a", "07.a", "08.a", "09.a", "10.a", "11.a", "12.a", "13.a", "14.a", "15.a", "16.a", "17.a", "18.a", "19.a", "20.a"],
    "Groups": [
        {
            "FullScore": 29,
            "TestIndices": {
                "Start": 0,
                "End": 15
            }
        },
        {
            "FullScore": 71,
            "TestIndices": {
                "Start": 15,
                "End": 20
            }
        }
    ],
	"CompileCommands":{
		"cpp": ["/usr/bin/c++","$USER_SRC", "-o", "$USER_BIN"],
		"python":[]
	}
}
```

## Checker
The checker script is run for each test case and the results are passed back to the grading script for grouping test cases.

The checker script must be provided by the user and takes in the following command-line arguments:
1. *Absolute* path to the input file of the test case
2. *Absolute* path to the output file generated by the user's program for the test case
3. *Absolute* path to the solution file of the test case

Of course, the checker script is passed to itself as the 0-th argument, but it can be safely ignored.

The checker must then write two lines to standard output. The first line denotes whether or not the user should receive the "Correct" verdict on the test case. If "Correct" is printed, then the user will be judged as "Correct" on the test case. Otherwise, if "Incorrect" is printed, then the user will received a "Wrong Answer" verdict. The second line must include a floating-point number indicating the user's score (*as a percentage*) on the test case. If the "Correct" verdict is given on the first line, then any score within the range [0, 100] is valid. Otherwise, if the "Incorrect" verdict is given on the first line, then the checker must print 0 on the second line.

For example,

```
Correct
75
```

and

```
Incorrect
0
```

are valid, but

```
Correct
1020
```

and

```
Incorrect
5
```

are invalid.

